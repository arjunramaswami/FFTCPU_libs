# FFTW

This folder contains the code to execute 3d FFT using FFTW. Currently executes
single and double precision complex FFT with the latter configurable to use multiple threads.

The program and bash scripts can be used to evaluate and consolidate performance of FFT3d using FFTW for different uniform sized transformations. Distinct backward or forward transformations can be executed a number of times to find the average performance using a multithreaded environment.

Link to the official FFTW [website](http://www.fftw.org/).

## Builds

### Prerequisites

- easybuild FFTW - `module load numlib/FFTW/3.3.8-gompi-2019b`
- C compiler that implements OpenMPI (tested with gcc 8.3.0) - `module load toolchain/gompi/2019b`

### Targets

1. Single precision single threaded FFTW

   `make` and `make sp` : Creates an executable named `host_sp` in the FFTW directory

2. For double precision single threaded FFTW

   `make dp` : Creates an executable named `host_dp` in the FFTW directory

3. Multi threaded FFTW (always uses double precision)

   `make omp` : Creates an executable named `host_omp` in the FFTW directory

Compiling with DEBUG macro set say, `make DEBUG=1 omp`, prints data input to
  FFTW

## Execution

Running the program with `-h` to print the options available.
Options:

```bash
  -m : FFT 1st Dim Size
  -n : FFT 2nd Dim Size
  -p : FFT 3rd Dim Size

  -i : Number of iterations of the FFT computation

  -t : number of threads in a multithreaded execution
  -b : toggle backward transformation
```

### Example multithreaded execution

```bash
  module load numlib/FFTW/3.3.8-gompi-2019b
  make omp
  ./host_omp -m 256 -n 256 -p 256 -t 40
  ./host_omp -m 256 -n 256 -p 256 -t 40 -b
```

## Interpreting Results

The output shows configuration of execution and the series of steps the program
executes followed by the consolidation of some performance metrics such as:

```bash
Number of runs : 100

FFT Size    Total Runtime(ms)   Avg Runtime(ms)     Throughput(GFLOPS)
   32          30.051               0.300               23.51
```

- FFT Size is the size of the 3d FFT
- Total Runtime (milliseconds) : total amount of time to execute the given
  number of iterations.
- Avg Runtime (milliseconds) : average runtime for a single iteration of execution i.e.,
  total runtime by the number of iterations
- Throughput (GFLOPS): Calculated by *3 * 5 * N * logN / (time for one FFT)* as described
  by the [FFTW Benchmark Methodology](http://www.fftw.org/speed/method.html).
  This is not the actual flop count rather an asymptotic measurement using the
  radix-2 Cooley Tukey algorithm.

### Important Points

- Runtime only measures the walltime of the FFTW execution, not the
  initialization and plan creation. Measured using `clock_gettime` to provide
  nanosecond resolution.
- Iterations are made on the same input data. Input data is [0, N^3 - 1] where
  N is the number of data points in a dimension.

## Results

### Best Runtime and Throughput

This is required to compare each FFT size with other FFT libraries and implementations.

| FFT3d Size | Best Runtime (ms) | Throughput (GFLOPS) |
|:----------:|:-----------------:|:-------------------:|
|     16     |   0.022           |     11.13           |
|     32     |   0.060           |     40.56           |
|     64     |   0.270           |     87.09           |
|     128    |   1.591           |    138.39           |
|     256    |    90.415         |     22.26           |

### Speedup with Multithreading

Maximum speedup obtained per size when strong scaling to 40 threads. 

| FFT3d Size | Max Speedup |
|:----------:|:-----------:|
|     16     |     1.0     |
|     32     |     3.34    |
|     64     |     10.67   |
|     128    |     24.48   |
|     256    |     10.58   |

#### Notes

- Better Speedup with increase in FFT size i.e., more data
- Could 256<sup>3</sup> offer better speedup with more threads?
- Can one estimate the maximum speedup possible?

### Details on execution

The bash file `omp_fftw_run.sh` can be used to execute on NOCTUA cluster :
```
sbatch omp_fftw_run.sh <array of sizes of fft3d>

sbatch omp_fftw_run.sh 16 32 64
```

1. loads `gcc/8.3.0` and `numlib/FFTW`

2. Sets openmp thread affinity env variables

3. Executes the application using srun on the same node from 1 to 40 threads
one after another and saves the output in distinct reports.

The bash file `create_csv.sh` can be used to create a csv output from the reports generated by the above bash file.

`./create_csv.sh <generated_report> <output.csv>`

## Configuring FFTW with OpenMP

Steps to configure multithreaded execution of FFTW with OpenMPI

### Code Modification

Initialize the environment:

  ```C
  #include<omp.h>
  int fftw_init_threads(void);
  ```

Make the plan with the necessary number of threads to execute

  ```C
  void fftw_plan_with_nthreads(int nthreads);
  ```

Execute with the normal API call

  ```C
  fftw_execute(plan)
  ```

Cleanup plan and threads after execution

  ```C
  fftw_destroy_plan()
  void fftw_cleanup_threads(void);
  ```

#### FFTW API for multithreading works only with double precision

- Creating a single precision plan after initializing threads produces single
   threaded outcomes.
- Creating fftwf alternatives throw linker error due to lack of such
   functionalities.

### Compilation

To compile with OpenMPI, link this additional flag `-lfftw3_omp` along with
`-fopenmp` other than the regular `fftw` flags. These are added to the makefile.

## Input Data and Validation

Generate a discrete signal of a single specific frequency by creating N<sup>3</sup> discrete points of a cosine and sine wave. Modify the frequency by creating its harmonic (positive multiple of a fundamental frequency) for variations.

Validate by checking the particular frequency's value after transformation.

### Error Bound

Why is the error bound calculated as $5 * \log _{2}(N1*N2*N3) * DBL\_EPSILSON$ or $FLT\_EPSILON$ ?

## Note

Execution is thread-safe but not plan creation and destruction, therefore use a
single thread for the latter.

## TODO

Pin OpenMP threads using environment variables as opposed to using `srun bind`.
There is also an environment variable to output the pinning results at runtime.

[Link1](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html)

[Link2](https://groups.uni-paderborn.de/pc2/lectures/hpccourse03/material/hpcadv.pdf)

